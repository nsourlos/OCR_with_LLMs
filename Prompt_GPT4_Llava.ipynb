{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\soyrl\\.conda\\envs\\cover_letter\\lib\\site-packages\\langchain\\llms\\openai.py:171: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n",
      "c:\\Users\\soyrl\\.conda\\envs\\cover_letter\\lib\\site-packages\\langchain\\llms\\openai.py:739: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "This task requires a script that interacts with the terminal, which is not possible to demonstrate here. However, I can provide a Python script that outlines the steps you need to follow. You can use the `subprocess` module in Python to interact with the terminal.\n",
       "\n",
       "Here's a rough script that should do what you're asking:\n",
       "\n",
       "```python\n",
       "import subprocess\n",
       "import time\n",
       "import os\n",
       "\n",
       "# Get list of images\n",
       "image_dir = '/home/soyrl/spyscape_test/'\n",
       "image_list = sorted([os.path.join(image_dir, img) for img in os.listdir(image_dir) if img.endswith('.jpg')])\n",
       "\n",
       "# Start the python command\n",
       "process = subprocess.Popen(['python', '-m', 'llava.serve.tcli', '--model-path', 'liuhaotian/llava-v1.6-vicuna-7b', '--load-4bit'], \n",
       "                           stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
       "\n",
       "for image in image_list:\n",
       "    # Wait for 'Image path:' prompt and send image path\n",
       "    while True:\n",
       "        output = process.stdout.readline().decode()\n",
       "        print(output)\n",
       "        if 'Image path:' in output:\n",
       "            process.stdin.write((image + '\\n').encode())\n",
       "            process.stdin.flush()\n",
       "            break\n",
       "\n",
       "    # Wait for 'USER:' prompt and send command\n",
       "    while True:\n",
       "        output = process.stdout.readline().decode()\n",
       "        print(output)\n",
       "        if 'USER:' in output:\n",
       "            process.stdin.write(('Extract the text in the image' + '\\n').encode())\n",
       "            process.stdin.flush()\n",
       "            break\n",
       "\n",
       "    # Wait for 30 seconds\n",
       "    time.sleep(30)\n",
       "\n",
       "    # Print 'ASSISTANT:' output\n",
       "    while True:\n",
       "        output = process.stdout.readline().decode()\n",
       "        print(output)\n",
       "        if 'ASSISTANT:' in output:\n",
       "            break\n",
       "\n",
       "    # Wait for 2 seconds\n",
       "    time.sleep(2)\n",
       "\n",
       "# Close the process\n",
       "process.stdin.close()\n",
       "process.terminate()\n",
       "process.wait(timeout=0.2)\n",
       "```\n",
       "\n",
       "This script assumes that the prompts ('Image path:', 'USER:', 'ASSISTANT:') are printed to stdout. If they are printed to stderr, you should read from `process.stderr` instead of `process.stdout`.\n",
       "\n",
       "Please note that this is a rough script and might need adjustments based on the actual behavior of the python command you're running."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import OpenAI\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv('env')) # read local .env file with OpenAI key\n",
    "import openai\n",
    "openai.api_key = openai_api_key=os.environ['OPENAI_API_KEY']\n",
    "\n",
    "llm=OpenAI(openai_api_key=openai_api_key,temperature=0,model_name='gpt-4') #Initialize LLM\n",
    "combined_letter=llm.predict(\"\"\"Given a a path with images (/home/soyrl/spyscape_test/), sort them and create a list of them. \n",
    "                            Run the python command (python -m llava.serve.tcli --model-path liuhaotian/llava-v1.6-vicuna-7b --load-4bit) in terminal.\n",
    "                             That command should be run first. After that, some messages will be printed in the terminal from each execution. \n",
    "                            Show them to the user. At some point terminal will show 'Image path:' and will wait for user input. \n",
    "                            Use the path of the first image in the list as input. Then the terminal will show 'USER:' and will wait for user input. \n",
    "                            Use as input there 'Extract the text in the image'. After that, you should wait for 30secs. \n",
    "                            Then, the terminal will show 'ASSISTANT:' with some text after it that you should show to the user.\n",
    "                             Wait 2 more seconds and then the 'Image Path:' will be shown again. Repeat the above process for all the images. \n",
    "                            Keep in mind that the python command should only be executed once at the beginning\"\"\") #Predict response using LLM\n",
    "\n",
    "Markdown(combined_letter)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cover_letter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
